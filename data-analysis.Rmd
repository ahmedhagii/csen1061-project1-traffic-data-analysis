---
title: "Traffic Data Analysis"
output: html_document
author: "Ahmed Akram"
---


```{r, message=FALSE}
library(dplyr)
library(ggplot2)
library(knitr)
library(stringr)
library(class)
```

### Data Processing

We'll load the data and take a look around its different columns and a sample of records.
```{r,  warning=FALSE}
Sys.setlocale(category="LC_ALL", locale="en_US.UTF-8")
data.raw <- read.csv('traffic-data.csv', encoding="UTF-8")
data.raw %>% glimpse
```

#### removing constant features

we have many features concerning the ads, let's see how many unique ad logos we got, just to make sure the ad features are not relevant to our investigation.

```{r,  warning=FALSE}
data.raw %>% select(ad.logo) %>% unique %>% nrow
```

since there's only one unique logo, then the ad is the same across all the rerports we got. Let's remove all features prefixed with "ad"

```{r}
data.reports <- data.raw %>% select(-(starts_with("ad")))
```

let's run this and get the count of unique elements for each feature
```{r,  warning=FALSE}
count_uniques <- sapply(data.reports, function(x) length(unique(x)))
```

now remove those with a 1 unique element
```{r,  warning=FALSE}
unwanted <- count_uniques[count_uniques == 1]
data.reports <- data.reports[, !names(data.reports) %in% names(unwanted)]
```


#### Investigating duplicates

let's try to find if we got any duplicates. A duplicate would occur if we crawled the same data, but at different times. So first I'll ignore the "crawl_date" column and compare using the rest.

```{r,  warning=FALSE}
data.no_date <- data.reports %>% select(-(crawl_date))
```

get all unique reports based only on the comment id, since each comment has a unique id (I suppose .. ?)

```{r,  warning=FALSE}
data.unique_comments_ids <- data.no_date %>% subset(!duplicated(rd.rp.cmid))
data.unique_comments_ids %>% nrow
```

now when I remove all the times features, we already removed the crawl date, but there are those columns telling us when the reprot was posted (posted 2 hours ago), this one will be represented by `rd.rp.hr` and `rd.rp.mn`. Morever there're `rd.hr` and `rd.mn`. I'm still not quite sure what they represent, but most probably the last time the whole line or road was updated. 

I'll remove all those for now and have the data with no times at all. 
```{r,  warning=FALSE}
data.no_times <- data.no_date %>% select(-rd.hr, -rd.mn, -rd.rp.hr, -rd.rp.mn)
```

Now if I were to run this, I expect to have the same number of records that I had in the `data.unique_comments_ids` dataframe.
```{r,  warning=FALSE}
data.no_times %>% unique %>% nrow
```

as you can see that doesn't happen, the later gave us `148367` while this one `209375`. I need to know which records are there with the same comment ids but differ in other columns. Let's investigate this a bit.


```{r,  warning=FALSE}
data.raw %>% count(rd.rp.cmid) %>% subset(n > 1) %>% arrange(-n)
```
grouping the data by the `cmid` and looking at them, shows that each group has a maximum of 4 comments grouped together. The below function just prints some of these groups.

```{r,  warning=FALSE}
fn <- function(data) {
    cm_ids <- data %>% count(rd.rp.cmid) %>% subset(n > 1) %>% arrange(-n) %>% select(rd.rp.cmid)
    cm_ids_arr <- cm_ids$rd.rp.cmid
    for(i in 1:5)
        print(data %>% subset(rd.rp.cmid == cm_ids_arr[i]) %>% select(rd.rp.cm, rd.rp.nm, rd.nm, rd.stid), width=Inf)
}
```

let's execute it:
```{r,  warning=FALSE}
fn(data.no_times %>% unique)
```

the printed data showed that each group of comments will have the same road name, same commenter, same comment text. My conclusion is that these reports were generated by the bey2ollak gps tracking feature, where I can turn it on and have my phone sending reports automatically.  

## Removing duplicates

```{r,  warning=FALSE}
data.unique <- data.reports %>% subset(!duplicated(rd.rp.cmid))
```



#### Reformating crawl_date

Format the crawl_date into date and adding 2 hours to accomodate for difference in timing between Cairo time and UTC time.
```{r,  warning=FALSE}
data.unique$crawl_date <- strptime(data.unique$crawl_date, "%a %b %d %H:%M:%S UTC %Y", tz="UTC")

data.unique$crawl_date <- data.unique$crawl_date + as.difftime(2, unit="hours")
```

#### Getting the real time of the report

Now to get the actual date of the report, we need to use the `rd.rp.hr` and `rd.rp.mn` columns. These two form the `1 hour ago`, `30 mins ago` and so on. So by performing crawl_hour - rd.rp.hr we should get the actual hour of posting the report.

```{r,  warning=FALSE}
data.unique$report_times <- (data.unique$crawl_date - as.difftime(as.numeric(data.unique$rd.rp.hr), unit="hours") - as.difftime(as.numeric(data.unique$rd.rp.mn), unit="mins"))

data.unique$report_hour <- data.unique$report_times %>% format("%H")
data.unique$report_minute <- data.unique$report_times %>% format("%M")
data.unique$report_day_name <- data.unique$report_times %>% format("%a")
```

#### Renaming the features
|              |                    |
|:------------:|:------------------:|
| rd.rp.cmid   | rerport.id         |
| rd.rp.cm     | report_comment     |
| rd.rp.stid   | report_status      |
| rd.rp.nm     | reporter_name      |
| rd.rp.fullnm | reporter_full_name |
| rd.ri        | road_id            |
| rd.stid      | road_status        |
| rd.nm        | road_name          |

```{r,  warning=FALSE}
data.unique <- data.unique %>% rename(
    report_comment=rd.rp.cm, 
    report_status_id=rd.rp.stid,
    report_id=rd.rp.cmid,
    reporter_name=rd.rp.nm,
    reporter_full_name=rd.rp.fullnm,
    road_id=rd.ri,
    road_status_id=rd.stid,
    road_name=rd.nm
)

data.unique <- data.unique %>% select(-rd.rp.hr, -rd.rp.mn, -rd.hr, -rd.mn)
```


```{r,  warning=FALSE}
cat(sort(data.unique%>%names()), sep="\n")
```

#### Identifying different statuses [lazeez, 7alawa ...]

by running this for each report_status, we can get the most frequent associated comment.

```{r,  warning=FALSE}
status <- function() {
  ret = c()
  for(id in 1:11) {
    status_word <- (data.unique %>% subset(report_status_id == id) %>% count(report_comment) %>% arrange(-n))[1,1]
    
    ret[id] = c(status_word, id)
  }
  ret
}

status()
```

|    |                     |
|:--:|:-------------------:|
| 1  | 7alawa              |
| 2  | lazeez              |
| 3  | mashy               |
| 4  | za7ma               |
| 5  | mafeesh amal        |
| 6  | Eigh el nezam?      |
| 7  | khatar-radar        |
| 8  | 7adsa               |
| 9  | 3otl                |
| 10 | GPS reporter status |

```{r,  warning=FALSE}
data.unique$report_hour <- as.numeric(data.unique$report_hour)

data.unique$report_status <- 
  data.unique$report_status_id %>% 
  sapply(function(x) {
    statuses <- c("7alawa", "lazeez", "mashy", "za7ma", "mafeesh amal", "Eigh el nezam?", "khatar-radar", "7adsa", "3otl", "GPS reporter status")
     
     statuses[x]
})
```


### Data Analysis

LEt's start by plotting some graphs ( finally :') ), we'll plot the hour of day against the no. of reports

```{r,  warning=FALSE}
p <- data.unique %>% ggplot() + stat_count(aes(x=report_hour, show_legend=FALSE), fill="firebrick2", colour="black" ,alpha=0.7, width=1) + xlab("Hour") + ylab("Count")

p
```

the results agrees with kind of an obvious insight that the rush hours would have the most number of reports. (8-10 AM and 4-6 PM)

Now let's plot the no. of reports against the report status 
```{r,  warning=FALSE}
png(file="statuses.png",width=400, height=350)
p <- data.unique %>% ggplot() + stat_count(aes(x=report_status, show_legend=FALSE, geom="point"), fill="blue", colour="cyan", alpha=0.7, width=0.96) + xlab("Hour") + ylab("Count")
dev.off()
p
```

this is really interesting, what I had in mind (and kinda makes sense) is that the negative report would be more thatn the positive, because supposedly I'll open the app and broadcast the status when I'm bored and stuck in traffic. However, as we can see it's totally the opposite. I still feel that something ain't right regarding that and will need more investigation. So let's dive in ...

## investigate the positive vs negative reports

let's group by status and hour, and then add a new feature counting how many reports in each hour and status
```{r,  warning=FALSE}
data.grouped <- data.unique %>% group_by_("report_status", "report_hour") %>% mutate(reports_per_hour_and_status=n())
```


```{r,  warning=FALSE}
p <- data.grouped %>% ggplot(aes(report_hour, reports_per_hour_and_status, col=report_status)) + scale_x_continuous(name="Hour", breaks=seq(0,23, 4)) + scale_y_continuous("No. of reports") + scale_color_discrete("Status")

p.scatter <- p + geom_point(alpha=0.6)

p.scatter.smooth <- p.scatter + geom_smooth(se=FALSE, span=0.1)

p.scatter.smooth
```


Let's extract the speed from bey2ollak's gps auto reporter

```{r,  warning=FALSE}
data.unique$speed <- data.unique %>% select(report_comment) %>% apply(1, function(x) {
    speed1 <- str_extract(x, "[0-9]+ km") %>% str_extract("[0-9]+") %>% as.numeric()
    
    if(is.na(speed1)[1]) {
        speed2 <- str_extract(x, "[0-9]+ كم") %>% str_extract("[0-9]+") %>% as.numeric()    
        speed_result <- speed2
    }else {
        speed_result <- speed1
    }
    
    speed_result
})
```

a quick summary of the speeds
```{r,  warning=FALSE}
data.unique$speed %>% summary
```

we need to remove the outliers 
```{r,  warning=FALSE}
data.unique %>% subset(speed > 200) %>% select(report_comment)
```

(shabab mostahter ....)

```{r,  warning=FALSE}
data.unique <- data.unique %>% subset(speed <= 200)
```


Let's see how many repors have a status of "GPS reporter status"
```{r,  warning=FALSE}
gps = "bey2ollakgps"
data.unique %>% subset(report_status_id == 10) %>% subset(reporter_name==gps) %>% nrow
```

and the following will get how many reports by bey2ollak gps have meaningful statuses [lazeez, 7alawa, za7ma ...]
```{r,  warning=FALSE}
 data.unique %>% subset(!is.na(report_status)) %>% subset(report_status_id != 10) %>% subset(reporter_name==gps) %>% select(report_comment, speed, report_status) %>% nrow
```

we need to classify those with status "GPS Report" into one of the other categories. First, let's boxplot the speed against the statuses. I guess we'll find a lot of outliers in the "GPS reporter status" as it contains a lot of repots that need to be categorized
```{r,  warning=FALSE}
data.unique %>% subset(report_status != "khatar-radar") %>% subset(!is.na(speed)) %>% subset(reporter_name==gps) %>% subset(!is.na(report_status)) %>% ggplot(aes(report_status, speed, col=report_status)) + geom_boxplot(alpha=0.7) +  scale_y_continuous("Speed") + scale_color_discrete("Status")
```

as expected we do have a lot of outliers. The "lazeez" status has some outliers into the negative part too. This could be explained by the fact that there're some roads that are not suitable for high speeds, but they're actually moving and not stuck, e.g. a car running 20 km/hr not because of the traffic, but rather because the road is not well done.
